---
title: "R Notebook"
output: html_notebook
---

```{r}
library(readr)
library(tidyverse)
library(dplyr)

library(ggplot2)
library(doBy)
library(DAAG)
library(caret)
library(glmnet)

options(scipen = 999)
```

```{r message=FALSE, warning=FALSE}
#if you do not have FinalData.csv generated already you must use redditsentiment.rmd to generate it on your local machine. The individual post and comment csv files should be pullable from git and found within their respective folders. 

#final dataset for reddit comment analysis 
#FinalData <- read_csv("RedditComments/FinalData.csv")
Data <- read_csv("FinalDataVaderSentiment.csv")
#Stock tickers that are mentioned at least 5 times per day on wallstreetbets - generated in RedditSentiment.rmd
WallStreetBetsTickers <- read_csv("RedditComments/WallStreetBetsTickers.csv")
#stock data final format for analysis and suitable for joining to Final Reddit Data (FinalData)
StockPricesFinal <- read_csv("RedditComments/StockPricesFinal.csv")
#lower casing to match FinalData format 
StockPricesFinal <- StockPricesFinal %>% mutate(stockTicker = tolower(stockTicker))
```

Building dataframe for modeling 
```{r}
ModelData <- Data %>% 
  select(created_date_time_EST,compound,neg,pos,ticker, link_id) %>% 
  group_by(link_id,ticker,Date = as.Date(created_date_time_EST)) %>% 
  summarise(sumNeg = sum(neg),
            sumPos = sum(pos),
            sumCompound = sum(compound),
            #ratioNeg = sum(neg)/sum(pos),
            count = n()
            ) %>% 
  inner_join(StockPricesFinal, by = c("Date" = "Date", "ticker" = "stockTicker") ) %>% 
  inner_join(loughranSentiment, by = c("link_id" = "link_id") ) %>% 
  select(-link_id) %>% 
  ungroup() %>%
  filter(ticker %in% tickers) %>% 
  mutate(
    ticker = as.factor(ticker)
  )
```

Split data by ticker
```{r}
split_data <- split(ModelData, ModelData$ticker) #use subet() with for loop?
#split_data$aapl
```


Model with all relevant variables
```{r}
all.data = ModelData[,-c(1,3,8)] #removing irrelevant columns
model.all = lm(stockPrice ~ ., data = all.data, na.action = na.exclude)

summary(model.all)


```

Cross Validation and Variable Selection 
```{r}
ctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 5)

lmFit_Step <- train(stockPrice ~ ., data = all.data, na.action = na.exclude, method = "lmStepAIC",
                    scope = list(lower = stockPrice~1, upper = stockPrice~.),
                    direction = "backward", trControl=ctrl)

```

Creating new dataframe with all sentiment variables selected (excluding historical stock prices)

```{r}
sent.data.cols = c("ticker","sumNeg","sumPos","sumCompound","count","constraining","litigious","negative",
              "positive","superfluous","uncertainty","stockPrice")

sent.data = all.data[,sent.data.cols]

```





Model selected through variable selection after starting with all variables
```{r}
lmFit_Step$finalModel
```

Variable experminetation

```{r}
mod_Step <- lm(stockPrice ~ ticker+positive+uncertainty,
              na.action = na.exclude, data = all.data)
summary(mod_Step)
```

Variable selection using dataset with all sentiment variables included
```{r}
ctrl2 <- trainControl(method = "repeatedcv", number = 5, repeats = 5)

lmFit_Step2 <- train(stockPrice ~ ., data = sent.data, na.action = na.exclude, method = "lmStepAIC",
                    scope = list(lower = stockPrice~1, upper = stockPrice~.),
                    direction = "backward", trControl=ctrl2)
```

```{r}
lmFit_Step2$finalModel
```

Exploring model created using variable selection with only sentiment variables
```{r}
mod_Step2 <- lm(stockPrice ~ ticker+sumNeg + sumPos + sumCompound + negative + positive,
              na.action = na.exclude, data = all.data)
summary(mod_Step2)
```





Selecting most common stock tickers from Reddit data
```{r}
ModelData %>% 
  count(ticker) %>% 
  arrange(desc(n))

#select only popular tickers
tickers <- c("mu","amd","tsla","snap","nvda","fb","amzn","baba","aapl","ge","msft","nflx")
```


Confidence Interval of sentiment model (95%)
```{r}
conf = confint(mod_Step, level = 0.95)
conf
```
```{r}
4/nrow(all.data)
```

Exploring influential points
```{r}
 
cook = cooks.distance(mod_Step)
cook[cook>(4/nrow(all.data))] #4/nrow(aapl_full) = 0.04819277
plot(cook)

```

Predicted values
```{r}
predicted_vals = predict.lm(mod_Step, type = "response")
```

Finding fitted and residual values from sentiment model
```{r}
library("car")
res = mod_Step2$residuals
fits = mod_Step2$fitted.values
#res

```


Exploring linear regression assumptions: Linearity
```{r}
par(mfrow=c(2,1))
plot(all.data$uncertainty, res, xlab="uncertainty",ylab="Residuals")
plot(all.data$positive, res, xlab="positive",ylab="Residuals")


```


Exploring linear regression assumptions: Constant Variance & Independence

```{r}
plot(fits, res, xlab="Fitted Values",ylab="Residuals")
```


Exploring linear regression assumptions: Normality
```{r}
qqPlot(res, ylab="Residuals")
```


Exploring linear regression assumptions: Normality
```{r}
hist(res, xlab="Residuals", main = "",nclass=10,col="orange")
```



