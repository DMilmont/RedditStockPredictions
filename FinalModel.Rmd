---
title: "R Notebook"
output: html_notebook
---

```{r}
library(readr)
library(tidyverse)

options(scipen = 999)
```

```{r message=FALSE, warning=FALSE}
#if you do not have FinalData.csv generated already you must use redditsentiment.rmd to generate it on your local machine. The individual post and comment csv files should be pullable from git and found within their respective folders. 

#final dataset for reddit comment analysis 
#FinalData <- read_csv("RedditComments/FinalData.csv")
Data <- read_csv("FinalDataVaderSentiment.csv")
#Stock tickers that are mentioned at least 5 times per day on wallstreetbets - generated in RedditSentiment.rmd
WallStreetBetsTickers <- read_csv("RedditComments/WallStreetBetsTickers.csv")
#stock data final format for analysis and suitable for joining to Final Reddit Data (FinalData)
StockPricesFinal <- read_csv("RedditComments/StockPricesFinal.csv")
#lower casing to match FinalData format 
StockPricesFinal <- StockPricesFinal %>% mutate(stockTicker = tolower(stockTicker))
```

Add Loughrain Sentiment and vader sentiment for final dataframe to be used in modeling 
```{r}
ModelData <- Data %>% 
  select(created_date_time_EST,compound,neg,pos,ticker, link_id) %>% 
  group_by(link_id,ticker,Date = as.Date(created_date_time_EST)) %>% 
  summarise(sumNeg = sum(neg),
            sumPos = sum(pos),
            sumCompound = sum(compound),
            #ratioNeg = sum(neg)/sum(pos),
            count = n()
            ) %>% 
  inner_join(StockPricesFinal, by = c("Date" = "Date", "ticker" = "stockTicker") ) %>% 
  inner_join(loughranSentiment, by = c("link_id" = "link_id") ) %>% 
  select(-link_id) %>% 
  ungroup() %>%
  filter(ticker %in% tickers) %>% 
  mutate(
    ticker = as.factor(ticker)
  )
```

train/test splits
```{r}
train <- ModelData %>% 
  filter(Date < '2018-07-01')

test <- ModelData %>% 
  filter(Date >= '2018-07-01')
```


models
```{r}
mod1 <- lm(pct.change2 ~ ticker + sumNeg + sumPos + sumCompound + constraining + litigious + negative + positive + superfluous + uncertainty + count, data = ModelData, na.action = na.exclude)
summary(mod1)

plot(mod1)

test$pred <- predict(mod1,test)

test$residuals <- test$pred - test$pct.change2



```
```{r}
mod1 <- lm(pct.change2 ~ ticker + sumNeg + sumPos + sumCompound + count, data = ModelData, na.action = na.exclude)
summary(mod1)
```


```{r}
modL <- lm(pct.change90 ~ ticker + constraining + negative + positive + count + negative * count, data = ModelData, na.action = na.exclude)
summary(modL)
```




Interaction term with volume and compound
```{r}
mod2 <- lm(pct.change2 ~ ticker + sumNeg * count, data = ModelData, na.action = na.exclude)
summary(mod2)
```



```{r}
rfData %>% 
  ggplot(aes(x=Date,y=sumNeg))
```


```{r}
ModelData %>% 
  count(ticker) %>% 
  arrange(desc(n))

#select only popular tickers
tickers <- c("mu","amd","tsla","snap","nvda","fb","amzn","baba","aapl","ge","msft","nflx")
```

```{r}
library(randomForest)

rfData <- ModelData %>% 
  filter(ticker %in% tickers) %>% 
  mutate(ticker = factor(ticker))

unique(rfData$ticker)

rf.mod <- randomForest(pct.change1 ~ ticker + sumNeg + sumPos + sumCompound + count, data = rfData, na.action = na.exclude, type = regression)

summary(rf.mod)

rf.mod$importance
```


```{r}
ModelData %>% 
  ggplot(aes(pct.change30)) + geom_histogram(binwidth = .01)
```

```{r}

```

