---
title: "Reddit Posts Sentiment Analysis"
output: html_notebook
---

```{r}
library(tidyverse)
library(tidytext)
library(stringr)
```

This notebook will obtain all stock data. You must have run cleanData.rmd first before moving onto this step. 

Stock tickers from TTR package - if you do not have this csv you can run the obtaining stock market tickers code chunk in getStockData.rmd
```{r}
library(readr)
stockSymbols <- read_csv("C:/Users/Dave/6242Project/RedditComments/stockTickers.csv")
```


Datframe that has link_id and stock ticker mentioned - this will strip and clean post titles and find posts that mention a stock ticker. Not all findings correspond to an actual stock ticker. For example the word play is used frequently in post titles when a user says something like: "What do you think of this stock play?" The user is actually referring to some specific stock move, not the company "play". This is motivation as to why we decided to manually create a subset of stock tickers that appear frequently in posts. 
```{r}
postSymbol <- posts %>% 
  select(title, link_id, created_date_time_EST) %>% 
  mutate(title = as.character(title),
         title = tolower(title),
         title = str_replace_all(title,"[[:punct:]]", "")) %>% 
  unnest_tokens(word,title) %>%
  anti_join(stop_words) %>% 
  semi_join(stockSymbols, by = c("word" = "Symbol"))

```

Finding only post link_id with one ticker in the post title - creating vector for filter purposes 
```{r}

distinctLinkPost <- postSymbol %>% 
  group_by(link_id) %>% 
  summarise(count = n()) %>% 
  filter(count == 1) %>% 
  select(link_id)

```


Create final dataframe of posts and comments filtered by posts with a single ticker in post title - this will also create a csv file as needed. 
```{r message=FALSE, warning=FALSE}

FinalData <- posts %>%
  semi_join(distinctLinkPost, by = c("link_id" = "link_id")) %>% 
  inner_join(comments, by = c("link_id")) 

FinalData <- FinalData %>% 
  left_join(postSymbol) %>% 
  rename(ticker = word)

#write.csv(FinalData, 'FinalData.csv', row.names = FALSE)

unique(FinalData$ticker)
```

FinalData characteristics - amount of comments in posts with single mention of stock ticker. 
54 x 308319

Posts with single mention of stock ticker:
20091
```{r}
length(FinalData)
nrow(FinalData)


```


We are most interested in stocks that are frequently talked about. This obtains the most talked about stocks on wall street bets. This produces 95 distinct stock tickers. 
```{r}
symbolCounts <- postSymbol %>% 
  group_by(Date = as.Date(created_date_time_EST), word) %>% 
  summarize(wordCount = n()) %>% 
  filter(wordCount > 1)

topSymbols <- unique(symbolCounts$word)

#write.csv(topSymbols,'WallStreetBetsTickers.csv',row.names = FALSE)
```


Plotting count of posts by stock ticker - using manually selected stock tickers that are most talked about on wallstreetbets 
```{r fig.height=20, fig.width=10, warning=FALSE}

tickers <- c("mu","amd","tsla","snap","nvda","fb","amzn","baba","aapl","ge","msft","nflx")

postSymbol %>% 
  group_by(Date = as.Date(created_date_time_EST), ticker = word) %>% 
  summarize(wordCount = n()) %>% 
  filter(ticker %in% tickers) %>% 
  ggplot(aes(x = Date, y = wordCount, group = ticker, color = ticker)) + 
  geom_line(alpha = 1, size = 1) + 
  scale_x_date(date_breaks = "1 month",date_labels = "%b") +
  facet_wrap(~ ticker, ncol = 2, scales = "free_y") +
  theme(legend.position="none") + 
  theme_minimal(base_size=15, base_family="Impact") +
  labs(title="Ticker Posts by Day",
       subtitle="Post counts with single ticker in title on r/wallstreetbets by day",
       x="Date (2018)",
       y="Post Count",
       caption=""
  )  + 
  theme(
    plot.subtitle = element_text(color="#AAAAAA", size=10),
    plot.title = element_text(family="Impact", size = 20),
    plot.caption = element_text(color="#AAAAAA", size=18)
  ) 


```


AMD posts and stock market volume - high correlation of 0.76 - this only works is single ticker stock data is pulled. 
```{r eval=FALSE, include=FALSE}
amdposts <- postSymbol %>% 
  group_by(Date = as.Date(created_date_time_EST), word) %>% 
  summarize(wordCount = n()) %>% 
  filter(word %in% c("amd")) %>% 
  inner_join(amd, by=c("Date" = "Index")) 


amdposts %>% 
  ggplot(aes(Date,wordCount, color = "red")) + 
  geom_line(alpha = 1, colour = 'red', size = 1) + 
  scale_x_date(date_breaks = "1 month",date_labels = "%b") +
  theme(legend.position="none") + 
  theme_minimal(base_size=15, base_family="Impact") +
  labs(title="Symbol Frequency Over Time",
       subtitle="",
       x="Date",
       y="Symbol Count",
       caption=""
  )  + 
  theme(
    plot.subtitle = element_text(color="#AAAAAA", size=10),
    plot.title = element_text(family="Impact", size = 20),
    plot.caption = element_text(color="#AAAAAA", size=18)
  ) 


amdposts %>% 
  ggplot(aes(Date,Volume, color = "red")) + 
  geom_line(alpha = 1, colour = 'red', size = 1) + 
  scale_x_date(date_breaks = "1 month",date_labels = "%b") +
  theme(legend.position="none") + 
  theme_minimal(base_size=15, base_family="Impact") +
  labs(title="Symbol Stock Market Volume",
       subtitle="",
       x="Date",
       y="Trading Volume",
       caption=""
  )  + 
  theme(
    plot.subtitle = element_text(color="#AAAAAA", size=10),
    plot.title = element_text(family="Impact", size = 20),
    plot.caption = element_text(color="#AAAAAA", size=18)
  ) 
```

```{r eval=FALSE, include=FALSE}
cor(amdposts$Volume,amdposts$wordCount)
```

Starting on the sentiment of comments for each post - this is the first method we tried. 
```{r}
commentSentiment <- comments %>% 
  select(link_id, created_date_time_EST, score, body) %>% 
  mutate(body = as.character(body),
         body = tolower(body),
         body = str_replace_all(body,"[[:punct:]]", "")) %>% 
  unnest_tokens(word,body) %>%
  anti_join(stop_words)
```


Building list of positive and negative words based on loughrain lexicon
```{r message=FALSE, warning=FALSE}

commentSentiment %>% 
  inner_join(get_sentiments("loughran"), by = "word") %>% 
  filter(sentiment == "negative" | sentiment == "positive") 
 


```



```{r fig.height=8, fig.width=8}
commentSentiment %>%
  count(word) %>%
  inner_join(get_sentiments("loughran"), by = "word") %>%
  group_by(sentiment) %>%
  top_n(5, n) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ sentiment, scales = "free") +
  ylab("Frequency of word in wallstreetbets comments and corresponding sentiment")
```

sentiment by ticker using loughrain lexicon
```{r}
postsTopSymbols <- postSymbol %>% 
  filter(word %in% tickers) %>% 
  mutate(ticker = word)

stock_sentiment_count <- commentSentiment %>%
  inner_join(get_sentiments("loughran"), by = "word") %>%
  inner_join(postsTopSymbols, by = "link_id") %>% 
  count(sentiment, ticker) %>%
  spread(sentiment, n, fill = 0)

#write.csv(stock_sentiment_count, "stock_sentiment_count.csv")
```

Creating loughrain sentiment 
```{r}
loughranSentiment <- commentSentiment %>%
  inner_join(get_sentiments("loughran"), by = "word") %>% 
  group_by(sentiment) %>% 
  ungroup()

loughranSentiment <- loughranSentiment %>% 
  group_by(link_id,sentiment) %>% 
  summarize(scoreTotal = sum(score)) %>% 
  ungroup() 

loughranSentiment <- loughranSentiment %>%
  spread(sentiment,scoreTotal) %>% 
  group_by(link_id) 

loughranSentiment <- loughranSentiment %>% 
  replace(is.na(.), 0)

# loughranSentiment <- loughranSentiment %>%
#   group_by(sentiment) %>%
#   mutate(i = row_number()) %>%
#   spread(sentiment,scoreTotal) %>%
#   select(-i)
# 
# loughranSentiment %>% 
#   group_by(link_id) %>% 
#   summarize()
  

# select(-V1) %>% 
#    group_by(V2) %>%
#    dplyr::mutate(i1 = row_number()) %>% 
#    spread(V2, V3) %>%
#    select(-i1)
# 
# loughranSentiment <- loughranSentiment %>% 
#   gather(key,value,-score,-sentiment) %>% 
#   group_by(sentiment) %>% 
#   select(value,sentiment,score)
# 
# loughranSentiment %>%
#   spread(sentiment,scoreTotal) %>% 
#   group_by(link_id) 
#   
#   
# StockPrices_Long <- StockPrices %>% 
#   gather(key,value, 1:94) %>% 
#   group_by(Date, key) %>% 
#   rename(stockTicker = key,
#          stockPrice = value
#  )  
```



```{r}
stock_sentiment_count %>%
  mutate(score = (positive - negative) / (positive + negative)) %>%
  mutate(ticker = reorder(ticker, score)) %>%
  ggplot(aes(ticker, score, fill = score > 0)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(x = "Ticker",
       y = "Positivity score among WallStreetBets Comments")
```



```{r}
posts %>% 
  group_by(as.Date(created_date_time_EST)) %>% 
  summarise(postsPerDay = n()) %>% 
  summarise(meanPostsPerDay = mean(postsPerDay))
```

```{r}
comments %>% 
  group_by(link_id) %>% 
  summarise(commentsPerLink = n()) %>% 
  summarise(meanCommentsPerPost = mean(commentsPerLink))
```
Obtaining stats on post comments 
```{r}
# postCommentsStats <- comments %>% 
#   select(body, link_id, created_date_time_EST) %>% 
#   mutate(body = as.character(body),
#          body = tolower(body),
#          body = str_replace_all(body,"[[:punct:]]", "")) %>% 
#   unnest_tokens(word,body)
```

```{r}
# postCommentsStats %>% 
#   group_by(link_id) %>% 
#   summarise(wordsPerComment = n()) %>% 
#   summarise(meanWordsPerPost = mean(wordsPerComment))

```

